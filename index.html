<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ProofGrader: Reliable Fine-Grained Evaluation of Natural Language Math Proofs</title>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300;400;500;600;700;800&family=Noto+Serif:wght@400;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Noto Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.75;
            color: #333;
            background: #fff;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 30px;
        }

        /* Header */
        header {
            padding: 80px 0 60px;
            text-align: center;
            background: #fff;
        }

        h1 {
            font-family: 'Noto Serif', Georgia, serif;
            font-size: 2.8em;
            font-weight: 700;
            line-height: 1.3;
            margin-bottom: 40px;
            color: #222;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
        }

        .authors {
            font-size: 1.05em;
            line-height: 1.8;
            color: #555;
            margin: 30px auto;
            max-width: 850px;
        }

        .authors strong {
            font-weight: 600;
            color: #333;
        }

        .affiliations {
            font-size: 0.95em;
            color: #777;
            margin-top: 20px;
        }

        .header-links {
            margin-top: 45px;
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-block;
            padding: 12px 28px;
            background: #4285f4;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-weight: 500;
            font-size: 0.95em;
            transition: background 0.2s;
            border: none;
        }

        .btn:hover {
            background: #3367d6;
        }

        .btn-secondary {
            background: #666;
        }

        .btn-secondary:hover {
            background: #555;
        }

        /* Sections */
        section {
            padding: 80px 0;
            border-bottom: 1px solid #e8e8e8;
        }

        h2 {
            font-family: 'Noto Serif', Georgia, serif;
            font-size: 2.2em;
            font-weight: 700;
            color: #222;
            margin-bottom: 30px;
            text-align: center;
        }

        h3 {
            font-size: 1.4em;
            margin: 40px 0 20px;
            color: #333;
            font-weight: 700;
        }

        p {
            font-size: 1.02em;
            color: #555;
            margin-bottom: 16px;
            line-height: 1.7;
        }

        /* Abstract */
        .abstract {
            background: #f9f9f9;
            padding: 40px 50px;
            margin: 30px 0;
            font-size: 1.05em;
            line-height: 1.75;
            color: #444;
            text-align: justify;
        }

        .abstract strong {
            color: #222;
            font-weight: 600;
        }

        /* Results Grid */
        .results-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 30px;
            margin: 50px 0;
        }

        .result-card {
            text-align: center;
            padding: 40px 20px;
            background: #fafafa;
            border-radius: 4px;
            transition: transform 0.2s;
        }

        .result-card:hover {
            transform: translateY(-4px);
        }

        .result-number {
            font-size: 3.5em;
            font-weight: 700;
            color: #4285f4;
            margin-bottom: 12px;
            line-height: 1;
        }

        .result-label {
            font-size: 0.9em;
            color: #666;
            font-weight: 500;
        }

        /* Visual Grid - 2x2 */
        .visual-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 40px;
            margin: 60px 0;
        }

        .visual-item {
            background: #fafafa;
            padding: 30px;
            border-radius: 4px;
        }

        .visual-title {
            font-size: 1.1em;
            font-weight: 600;
            color: #333;
            margin-bottom: 20px;
            text-align: center;
        }

        .chart-container {
            width: 100%;
            margin: 20px 0;
        }

        .chart-container svg {
            width: 100%;
            height: auto;
        }

        .visual-caption {
            font-size: 0.92em;
            color: #666;
            margin-top: 15px;
            text-align: center;
        }

        .visual-caption strong {
            color: #333;
        }

        /* Stats Row */
        .stats-row {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 30px;
            margin: 50px 0;
        }

        .stat-box {
            background: #fafafa;
            padding: 30px;
            border-radius: 4px;
            border-top: 3px solid #4285f4;
        }

        .stat-box strong {
            font-size: 1.25em;
            color: #333;
            font-weight: 700;
            display: block;
            margin-bottom: 8px;
        }

        .stat-box p {
            margin: 0;
            font-size: 0.95em;
            color: #666;
        }

        /* Contributions List */
        .contributions {
            list-style: none;
            padding: 0;
            margin: 40px 0;
        }

        .contributions li {
            padding: 25px 30px;
            margin: 20px 0;
            background: #fafafa;
            border-radius: 4px;
            border-left: 3px solid #4285f4;
        }

        .contributions li strong {
            color: #333;
            font-weight: 700;
            font-size: 1.1em;
        }

        /* Dataset section */
        .dataset-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
            margin: 40px 0;
        }

        .dataset-stat {
            background: #fafafa;
            padding: 30px 20px;
            text-align: center;
            border-radius: 4px;
        }

        .dataset-stat-number {
            font-size: 2.8em;
            font-weight: 700;
            color: #4285f4;
            margin-bottom: 10px;
        }

        .dataset-stat-label {
            font-size: 0.9em;
            color: #666;
        }

        .competitions {
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            justify-content: center;
            margin: 30px 0;
        }

        .competition-badge {
            background: #f0f0f0;
            padding: 8px 18px;
            border-radius: 4px;
            font-weight: 500;
            font-size: 0.95em;
            color: #555;
        }

        /* Highlight box */
        .highlight {
            background: #fffbf0;
            padding: 30px;
            margin: 40px 0;
            border-left: 3px solid #ffa500;
            border-radius: 4px;
        }

        .highlight p {
            margin: 0;
            color: #555;
        }

        .highlight strong {
            color: #333;
        }

        /* Footer */
        footer {
            background: #fafafa;
            padding: 60px 0;
            text-align: center;
            border-top: 1px solid #e8e8e8;
        }

        footer p {
            color: #666;
            margin-bottom: 10px;
        }

        footer a {
            color: #4285f4;
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        /* Responsive */
        @media (max-width: 968px) {
            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.8em;
            }

            .results-grid {
                grid-template-columns: repeat(2, 1fr);
            }

            .visual-grid {
                grid-template-columns: 1fr;
            }

            .dataset-grid {
                grid-template-columns: repeat(2, 1fr);
            }

            .stats-row {
                grid-template-columns: 1fr;
            }

            .abstract {
                padding: 30px;
            }
        }

        hr {
            border: none;
            border-top: 1px solid #e8e8e8;
            margin: 60px 0;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Reliable Fine-Grained Evaluation of Natural Language Math Proofs</h1>
            <div class="authors">
                <strong>Wenjie Ma</strong><sup>1</sup>, 
                Andrei Cojocaru<sup>1</sup>, 
                Neel Kolhe<sup>1</sup>, 
                Bradley Louie<sup>1</sup>, 
                Robin Said Sharif<sup>1</sup>,
                Haihan Zhang<sup>3</sup>, 
                Vincent Zhuang<sup>2</sup>, 
                Matei Zaharia<sup>1</sup>, 
                <strong>Sewon Min</strong><sup>1,4</sup>
            </div>
            <div class="affiliations">
                <sup>1</sup>UC Berkeley &nbsp;·&nbsp; 
                <sup>2</sup>Google DeepMind &nbsp;·&nbsp; 
                <sup>3</sup>Peking University &nbsp;·&nbsp; 
                <sup>4</sup>Allen Institute for AI
            </div>
            <div class="header-links">
                <a href="paper.pdf" class="btn">Paper</a>
                <a href="https://huggingface.co/datasets/wenjiema02/ProofBench" class="btn">Dataset</a>
                <a href="viewer.html" class="btn">Viewer</a>
                <a href="#" class="btn btn-secondary" style="opacity: 0.5; cursor: default; pointer-events: none;">Code (Coming Soon)</a>
            </div>
        </div>
    </header>

    <section id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <div class="abstract">
                Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0–7 scale to model-generated math proofs. We introduce <strong>ProofBench</strong>, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. Our analysis delivers <strong>ProofGrader</strong>, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of <strong>0.926</strong> against expert scores. Finally, we demonstrate its practical utility in a best-of-<em>n</em> selection task: at <em>n</em>=16, ProofGrader achieves an average score of 4.14/7, closing <strong>78%</strong> of the gap between a naive binary evaluator (2.48) and the human oracle (4.62).
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="results-grid">
                <div class="result-card">
                    <div class="result-number">0.93</div>
                    <div class="result-label">Mean Absolute Error</div>
                </div>
                <div class="result-card">
                    <div class="result-number">78%</div>
                    <div class="result-label">Gap Closed</div>
                </div>
                <div class="result-card">
                    <div class="result-number">145</div>
                    <div class="result-label">Problems</div>
                </div>
                <div class="result-card">
                    <div class="result-number">435</div>
                    <div class="result-label">Annotations</div>
                </div>
            </div>
        </div>
    </section>

    <section id="results">
        <div class="container">
            <h2>Key Results</h2>
            
            <p style="max-width: 900px; margin: 0 auto 30px; text-align: center;">
                Our systematic exploration of the evaluator design space reveals clear trends and produces a high-performing evaluator. Below, we highlight key findings from our experiments and demonstrate ProofGrader's practical utility.
            </p>

            <h3>Impact of Design Choices</h3>
            <p>The left chart shows how different context configurations affect evaluator performance. ProofGrader (with both marking scheme and reference solutions) achieves the lowest MAE of 0.926, while providing no context results in poor performance (MAE of 1.68). The right chart demonstrates ProofGrader's practical value: as we increase the number of candidate solutions from 1 to 16, ProofGrader consistently selects high-quality proofs, closely tracking the human oracle and far outperforming binary and baseline methods.</p>
            
            <div class="visual-grid">
                <!-- Chart 1: Evaluator Performance -->
                <div class="visual-item">
                    <div class="visual-title">Impact of Context on Evaluator Quality</div>
                    <div class="chart-container">
                        <svg viewBox="0 0 300 180">
                            <rect x="40" y="140" width="50" height="100" fill="#4285f4"/>
                            <rect x="110" y="90" width="50" height="150" fill="#aaa"/>
                            <rect x="180" y="60" width="50" height="180" fill="#ccc"/>
                            <rect x="250" y="30" width="50" height="210" fill="#e0e0e0"/>
                            
                            <line x1="30" y1="20" x2="30" y2="240" stroke="#999" stroke-width="1.5"/>
                            <line x1="30" y1="240" x2="310" y2="240" stroke="#999" stroke-width="1.5"/>
                            
                            <text x="65" y="260" text-anchor="middle" font-size="11" fill="#333" font-weight="600">ProofGrader</text>
                            <text x="135" y="260" text-anchor="middle" font-size="10" fill="#666">+MS</text>
                            <text x="205" y="260" text-anchor="middle" font-size="10" fill="#666">+Ref</text>
                            <text x="275" y="260" text-anchor="middle" font-size="10" fill="#666">None</text>
                            
                            <text x="65" y="130" text-anchor="middle" font-size="14" fill="#fff" font-weight="700">0.93</text>
                            <text x="135" y="80" text-anchor="middle" font-size="13" fill="#333" font-weight="600">1.07</text>
                            <text x="205" y="50" text-anchor="middle" font-size="13" fill="#333" font-weight="600">1.33</text>
                            <text x="275" y="20" text-anchor="middle" font-size="13" fill="#333" font-weight="600">1.68</text>
                            
                            <text x="15" y="15" font-size="10" fill="#999">MAE</text>
                        </svg>
                    </div>
                    <p class="visual-caption"><strong>0.926 MAE</strong> — Adding context dramatically improves performance</p>
                </div>

                <!-- Chart 2: Best-of-N -->
                <div class="visual-item">
                    <div class="visual-title">Best-of-N Selection Performance</div>
                    <div class="chart-container">
                        <svg viewBox="0 0 340 280">
                            <!-- Grid -->
                            <line x1="50" y1="200" x2="310" y2="200" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="50" y1="155" x2="310" y2="155" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="50" y1="110" x2="310" y2="110" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="50" y1="65" x2="310" y2="65" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="50" y1="20" x2="310" y2="20" stroke="#f0f0f0" stroke-width="1"/>
                            
                            <!-- Axes -->
                            <line x1="50" y1="20" x2="50" y2="200" stroke="#999" stroke-width="1.5"/>
                            <line x1="50" y1="200" x2="310" y2="200" stroke="#999" stroke-width="1.5"/>
                            
                            <!-- Y-axis labels -->
                            <text x="42" y="204" text-anchor="end" font-size="9" fill="#999">2.0</text>
                            <text x="42" y="159" text-anchor="end" font-size="9" fill="#999">2.5</text>
                            <text x="42" y="114" text-anchor="end" font-size="9" fill="#999">3.0</text>
                            <text x="42" y="69" text-anchor="end" font-size="9" fill="#999">3.5</text>
                            <text x="42" y="24" text-anchor="end" font-size="9" fill="#999">4.0</text>
                            <text x="42" y="-21" text-anchor="end" font-size="9" fill="#999">4.5</text>
                            
                            <!-- X-axis labels -->
                            <text x="50" y="215" text-anchor="middle" font-size="9" fill="#999">1</text>
                            <text x="115" y="215" text-anchor="middle" font-size="9" fill="#999">4</text>
                            <text x="180" y="215" text-anchor="middle" font-size="9" fill="#999">8</text>
                            <text x="245" y="215" text-anchor="middle" font-size="9" fill="#999">12</text>
                            <text x="310" y="215" text-anchor="middle" font-size="9" fill="#999">16</text>
                            
                            <text x="180" y="230" text-anchor="middle" font-size="10" fill="#666">n (number of candidates)</text>
                            
                            <!-- Lines -->
                            <polyline points="50,200 67.3,200 84.7,201 102,202 119,201 136,201 154,201 171,201 188,201 206,201 223,201 240,201 258,201 275,201 292,201 310,201" 
                                      fill="none" stroke="#ddd" stroke-width="2" stroke-dasharray="4,2"/>
                            <polyline points="50,200 67.3,180 84.7,169 102,165 119,162 136,160 154,159 171,158 188,159 206,159 223,157 240,159 258,158 275,158 292,157 310,157" 
                                      fill="none" stroke="#999" stroke-width="2"/>
                            <polyline points="50,200 67.3,171 84.7,154 102,144 119,140 136,133 154,131 171,128 188,127 206,125 223,123 240,122 258,120 275,119 292,118 310,116" 
                                      fill="none" stroke="#888" stroke-width="2"/>
                            <polyline points="50,200 67.3,151 84.7,127 102,113 119,103 136,96 154,90 171,87 188,83 206,84 223,82 240,83 258,83 275,84 292,85 310,88" 
                                      fill="none" stroke="#93c5fd" stroke-width="2"/>
                            <polyline points="50,200 67.3,144 84.7,116 102,108 119,92 136,82 154,74 171,68 188,65 206,63 223,61 240,58 258,58 275,56 292,56 310,55" 
                                      fill="none" stroke="#60a5fa" stroke-width="2"/>
                            <polyline points="50,200 67.3,146 84.7,113 102,94 119,81 136,71 154,64 171,59 188,56 206,53 223,52 240,49 258,49 275,47 292,46 310,44" 
                                      fill="none" stroke="#3b82f6" stroke-width="2"/>
                            <polyline points="50,200 67.3,143 84.7,110 102,92 119,77 136,66 154,58 171,53 188,49 206,47 223,44 240,42 258,40 275,38 292,36 310,34" 
                                      fill="none" stroke="#4285f4" stroke-width="3.5"/>
                            <polyline points="50,200 67.3,133 84.7,94 102,76 119,63 136,52 154,43 171,37 188,32 206,28 223,25 240,22 258,20 275,17 292,15 310,13" 
                                      fill="none" stroke="#34a853" stroke-width="3.5"/>
                            
                            <!-- Legend - Horizontal layout below x-axis label -->
                            <!-- Row 1 -->
                            <g transform="translate(15, 240)">
                                <line x1="0" y1="0" x2="14" y2="0" stroke="#34a853" stroke-width="3"/>
                                <text x="17" y="4" font-size="8.5" fill="#333" font-weight="600">Human Oracle</text>
                                
                                <line x1="90" y1="0" x2="104" y2="0" stroke="#4285f4" stroke-width="3"/>
                                <text x="107" y="4" font-size="8.5" fill="#4285f4" font-weight="600">ProofGrader</text>
                                
                                <line x1="175" y1="0" x2="189" y2="0" stroke="#3b82f6" stroke-width="2.5"/>
                                <text x="192" y="4" font-size="8" fill="#666" font-weight="600">Fine-grained (Ref+MS)</text>
                            </g>
                            <!-- Row 2 -->
                            <g transform="translate(15, 252)">
                                <line x1="0" y1="0" x2="14" y2="0" stroke="#60a5fa" stroke-width="2.5"/>
                                <text x="17" y="4" font-size="8" fill="#666" font-weight="600">Fine-grained (Ref)</text>
                                
                                <line x1="110" y1="0" x2="124" y2="0" stroke="#93c5fd" stroke-width="2.5"/>
                                <text x="127" y="4" font-size="8" fill="#666" font-weight="600">Fine-grained (None)</text>
                                
                                <line x1="220" y1="0" x2="234" y2="0" stroke="#888" stroke-width="2.5"/>
                                <text x="237" y="4" font-size="8" fill="#666" font-weight="600">Binary (Ref)</text>
                            </g>
                            <!-- Row 3 -->
                            <g transform="translate(15, 264)">
                                <line x1="0" y1="0" x2="14" y2="0" stroke="#999" stroke-width="2"/>
                                <text x="17" y="4" font-size="8" fill="#666" font-weight="600">Binary (None)</text>
                                
                                <line x1="85" y1="0" x2="99" y2="0" stroke="#cbd5e1" stroke-width="2" stroke-dasharray="3,2"/>
                                <text x="102" y="4" font-size="8" fill="#666" font-weight="600">Baseline</text>
                            </g>
                            
                            <!-- Endpoint highlight -->
                            <circle cx="310" cy="34" r="4" fill="#4285f4" stroke="#fff" stroke-width="2"/>
                        </svg>
                    </div>
                    <p class="visual-caption">Closes <strong>78% of gap</strong> to human oracle at n=16</p>
                </div>
            </div>

            <h3 style="margin-top: 50px;">Dataset Insights</h3>
            <p>The charts below show how different state-of-the-art models perform on ProofBench and the structure of our dataset. Even the best models (OpenAI o3) achieve a mean score of only 2.87/7, highlighting substantial room for improvement in proof generation. Our dataset provides a balanced mix of problems across competitions and comprehensive coverage through multiple model generators.</p>

            <div class="visual-grid">
                <!-- Chart 3: Generator Performance -->
                <div class="visual-item">
                    <div class="visual-title">Generator Performance on ProofBench</div>
                    <div class="chart-container">
                        <svg viewBox="0 0 300 200">
                            <line x1="60" y1="180" x2="280" y2="180" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="60" y1="140" x2="280" y2="140" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="60" y1="100" x2="280" y2="100" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="60" y1="60" x2="280" y2="60" stroke="#f0f0f0" stroke-width="1"/>
                            <line x1="60" y1="20" x2="280" y2="20" stroke="#f0f0f0" stroke-width="1"/>
                            
                            <line x1="60" y1="20" x2="60" y2="180" stroke="#999" stroke-width="1.5"/>
                            <line x1="60" y1="180" x2="280" y2="180" stroke="#999" stroke-width="1.5"/>
                            
                            <rect x="80" y="109" width="50" height="71" fill="#ea4335"/>
                            <rect x="145" y="119" width="50" height="61" fill="#fbbc04"/>
                            <rect x="210" y="144" width="50" height="36" fill="#34a853"/>
                            
                            <text x="105" y="100" text-anchor="middle" font-size="14" fill="#ea4335" font-weight="700">2.87</text>
                            <text x="170" y="110" text-anchor="middle" font-size="14" fill="#fbbc04" font-weight="700">2.53</text>
                            <text x="235" y="137" text-anchor="middle" font-size="14" fill="#34a853" font-weight="700">1.83</text>
                            
                            <text x="105" y="195" text-anchor="middle" font-size="11" fill="#333" font-weight="600">o3</text>
                            <text x="170" y="195" text-anchor="middle" font-size="11" fill="#333" font-weight="600">Gemini</text>
                            <text x="235" y="195" text-anchor="middle" font-size="11" fill="#333" font-weight="600">R1</text>
                            
                            <text x="50" y="184" text-anchor="end" font-size="9" fill="#999">0</text>
                            <text x="50" y="144" text-anchor="end" font-size="9" fill="#999">1</text>
                            <text x="50" y="104" text-anchor="end" font-size="9" fill="#999">2</text>
                            <text x="50" y="64" text-anchor="end" font-size="9" fill="#999">3</text>
                        </svg>
                    </div>
                    <p class="visual-caption"><strong>OpenAI o3</strong> leads with 2.87/7, followed by Gemini</p>
                </div>

                <!-- Chart 4: Dataset -->
                <div class="visual-item">
                    <div class="visual-title">ProofBench Dataset</div>
                    <div class="chart-container">
                        <svg viewBox="0 0 300 200">
                            <circle cx="150" cy="100" r="50" fill="#e8f0fe" stroke="#4285f4" stroke-width="3"/>
                            <text x="150" y="95" text-anchor="middle" font-size="28" fill="#4285f4" font-weight="700">435</text>
                            <text x="150" y="115" text-anchor="middle" font-size="11" fill="#666" font-weight="600">Solutions</text>
                            
                            <circle cx="75" cy="40" r="25" fill="#ea4335"/>
                            <text x="75" y="45" text-anchor="middle" font-size="16" fill="#fff" font-weight="700">145</text>
                            <text x="75" y="75" text-anchor="middle" font-size="9" fill="#666">Problems</text>
                            <line x1="95" y1="55" x2="125" y2="80" stroke="#ddd" stroke-width="2"/>
                            
                            <circle cx="225" cy="40" r="25" fill="#fbbc04"/>
                            <text x="225" y="45" text-anchor="middle" font-size="16" fill="#fff" font-weight="700">6</text>
                            <text x="225" y="75" text-anchor="middle" font-size="9" fill="#666">Contests</text>
                            <line x1="205" y1="55" x2="175" y2="80" stroke="#ddd" stroke-width="2"/>
                            
                            <circle cx="75" cy="160" r="25" fill="#34a853"/>
                            <text x="75" y="165" text-anchor="middle" font-size="16" fill="#fff" font-weight="700">3</text>
                            <text x="75" y="195" text-anchor="middle" font-size="9" fill="#666">Models</text>
                            <line x1="95" y1="145" x2="125" y2="120" stroke="#ddd" stroke-width="2"/>
                            
                            <circle cx="225" cy="160" r="25" fill="#9334e9"/>
                            <text x="225" y="165" text-anchor="middle" font-size="16" fill="#fff" font-weight="700">0-7</text>
                            <text x="225" y="195" text-anchor="middle" font-size="9" fill="#666">Scale</text>
                            <line x1="205" y1="145" x2="175" y2="120" stroke="#ddd" stroke-width="2"/>
                        </svg>
                    </div>
                    <p class="visual-caption"><strong>Expert-annotated</strong> fine-grained ratings</p>
                </div>
            </div>

            <div class="stats-row">
                <div class="stat-box">
                    <strong>76.5% within ±1</strong>
                    <p>ProofGrader predictions align closely with expert scores</p>
                </div>
                <div class="stat-box" style="border-top-color: #34a853;">
                    <strong>0.008 bias</strong>
                    <p>Nearly unbiased evaluation across all test cases</p>
                </div>
                <div class="stat-box" style="border-top-color: #9334e9;">
                    <strong>Outperforms pairwise</strong>
                    <p>More efficient than tournament selection strategies</p>
                </div>
            </div>

            <h3 style="margin-top: 60px;">Downstream Validation: Best-of-N Selection</h3>
            <p>Beyond offline metrics, we validate ProofGrader's practical utility through a best-of-<em>n</em> selection task—a standard proxy for evaluating reward model quality. Given <em>n</em> candidate proofs for each problem, the evaluator must identify the highest-quality solution. This task directly mirrors real-world applications like rejection sampling for data generation and reward modeling for reinforcement learning.</p>
            
            <p style="margin-top: 20px;">At <em>n</em>=16, ProofGrader achieves an average expert score of 4.14/7 for selected proofs, compared to 2.48 for a naive binary evaluator and 4.62 for the human oracle. This closes 78% of the performance gap, demonstrating that ProofGrader can nearly match human performance in identifying high-quality proofs—a critical capability for training future proof generators.</p>

            <p style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-radius: 4px; font-style: italic; color: #666;">
                → For additional experiments including comparison with tournament-style pairwise selection methods and analysis across different generator models, see <strong>Section 4</strong> of the paper.
            </p>
        </div>
    </section>

    <section id="why">
        <div class="container">
            <h2>Why Is This Important?</h2>
            
            <p style="max-width: 900px; margin: 0 auto 25px; text-align: center; font-size: 1.12em;">
                While LLMs have made remarkable progress in mathematical reasoning, evaluating natural language proofs remains a critical unsolved challenge.
            </p>

            <p style="max-width: 900px; margin: 0 auto 35px;">
                <strong>The Core Problem:</strong> Many proof problems either don't have a single verifiable final answer, or require assessing the reasoning process itself—not just the conclusion. For example, problems that ask to "prove" a statement without specifying a unique answer, or problems where an incorrect proof might accidentally arrive at the right answer.
            </p>

            <h3>Current Approaches Fall Short</h3>
            <p>Existing methods for proof evaluation each have significant limitations:</p>
            <ul style="margin-left: 25px; margin-bottom: 30px; line-height: 1.8;">
                <li><strong>Human grading</strong> is accurate but slow and expensive, making it impractical for training models at scale.</li>
                <li><strong>Formal verification</strong> (e.g., Lean) offers certainty but is detached from the natural language used in education and research. Automatically translating natural-language proofs into formal systems remains extremely challenging.</li>
                <li><strong>LLM-as-judge</strong> approaches show promise but lack reliability—their judgments are highly sensitive to model choice, prompt design, and rubric construction, with no principled methodology for optimization.</li>
            </ul>

            <h3>Why Reliable Evaluation Matters</h3>
            <p>A high-quality proof evaluator is essential for advancing the field because it:</p>
            <ul style="margin-left: 25px; line-height: 1.8;">
                <li>Provides <strong>faithful assessments</strong> of model capabilities for benchmarking progress</li>
                <li>Enables <strong>accurate reward signals</strong> for training proof generators via reinforcement learning</li>
                <li>Supports <strong>downstream applications</strong> like best-of-n selection and rejection sampling for data generation</li>
                <li>Bridges the gap between <strong>formal verification and natural language</strong> reasoning</li>
            </ul>

            <p style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-radius: 4px; font-style: italic; color: #666;">
                → For a detailed discussion of the limitations of existing approaches and our motivation, see <strong>Section 1</strong> of the paper.
            </p>
        </div>
    </section>

    <section id="methodology">
        <div class="container">
            <h2>Methodology</h2>
            
            <p style="font-size: 1.15em; max-width: 850px; margin: 0 auto 40px; text-align: center; color: #444;">
                Rather than training new models, we systematically explore the design space of evaluators to find the best configuration. Our approach is entirely <strong>training-free</strong>, making it practical and reproducible.
            </p>

            <p style="max-width: 900px; margin: 0 auto 30px;">
                We conduct controlled experiments across four key design axes, testing different combinations to understand what makes an evaluator reliable. Here's a summary of our findings:
            </p>
            
            <ul class="contributions">
                <li>
                    <strong>1. Backbone Model:</strong> We tested six different LLMs as evaluator backbones, ranging from GPT-4o to o3. Our experiments reveal that the choice of backbone is the <em>single most important factor</em>—stronger reasoning models consistently produce better evaluations across all metrics. OpenAI o3 emerged as the best performer.
                </li>
                <li>
                    <strong>2. Context Design:</strong> We compared four context configurations: providing both reference solutions and marking schemes, providing only one or the other, or providing neither. The marking scheme proves crucial—it provides the majority of the performance gain and helps the evaluator understand what constitutes progress in a proof, preventing it from over-crediting fluent but incorrect reasoning.
                </li>
                <li>
                    <strong>3. Instruction Style:</strong> We tested three instruction strategies for how evaluators should use the provided context. Interestingly, the optimal instruction style depends on the backbone's capabilities: stronger models (like o3) benefit from flexible instructions that allow them to recognize valid alternative approaches, while mid-tier models require more prescriptive guidance to avoid over-crediting.
                </li>
                <li>
                    <strong>4. Workflow Design:</strong> Beyond single-pass evaluation, we explored ensembling (running the evaluator multiple times and aggregating scores) and multi-stage pipelines (decomposing evaluation into sequential steps). We found that simple ensembling—taking the median of five independent runs—consistently improves accuracy while reducing variance.
                </li>
            </ul>

            <p style="margin-top: 30px;"><strong>Key Takeaway:</strong> Our analysis shows that design factors have dramatically different impact magnitudes: <em>backbone model</em> ≫ <em>context</em> ≫ <em>instruction style</em>. Based on this, we recommend: (1) use the strongest available reasoning model, (2) always include a problem-specific marking scheme, and (3) match instruction rigidity to the model's capability level.</p>

            <h3 style="margin-top: 50px;">ProofGrader: Our Best Evaluator</h3>
            <p>Combining insights from our systematic study, <strong>ProofGrader</strong> uses OpenAI o3 as the backbone, provides both reference solutions and marking schemes as context, employs flexible instructions that allow for alternative solution paths, and ensembles five independent evaluation runs using median aggregation.</p>
            
            <div class="highlight" style="margin-top: 25px;">
                <p><strong>Performance:</strong> ProofGrader achieves an MAE of 0.926 against expert scores, with 76.5% of predictions within ±1 point and nearly zero bias (0.008). This significantly outperforms naive baselines and demonstrates strong alignment with human judgment.</p>
            </div>

            <p style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-radius: 4px; font-style: italic; color: #666;">
                → For detailed ablation studies, statistical significance tests, and analysis of per-generator performance, see <strong>Section 3</strong> of the paper.
            </p>
        </div>
    </section>

    <section id="dataset">
        <div class="container">
            <h2>ProofBench Dataset</h2>
            
            <p style="text-align: center; max-width: 850px; margin: 0 auto 40px; font-size: 1.15em;">
                To enable our systematic study, we introduce <strong>ProofBench</strong>—the first expert-annotated dataset of fine-grained proof ratings spanning multiple prestigious mathematics competitions and recent years.
            </p>

            <div class="dataset-grid">
                <div class="dataset-stat">
                    <div class="dataset-stat-number">145</div>
                    <div class="dataset-stat-label">Problems</div>
                </div>
                <div class="dataset-stat">
                    <div class="dataset-stat-number">435</div>
                    <div class="dataset-stat-label">Solutions</div>
                </div>
                <div class="dataset-stat">
                    <div class="dataset-stat-number">6</div>
                    <div class="dataset-stat-label">Competitions</div>
                </div>
                <div class="dataset-stat">
                    <div class="dataset-stat-number">3</div>
                    <div class="dataset-stat-label">SOTA Models</div>
                </div>
            </div>

            <p style="max-width: 900px; margin: 40px auto 30px;">
                We collected problems from six major mathematics competitions—USAMO, IMO, Putnam, USA TST, APMO, and EGMO—spanning 2022 to 2025. For each problem, we generated solutions using three state-of-the-art reasoning models: OpenAI o3, Gemini-2.5-Pro, and DeepSeek-R1-0528. This selection ensures diversity across both proprietary and open-source model families.
            </p>

            <h3 style="text-align: center; margin-top: 50px;">Competitions Covered</h3>
            <div class="competitions">
                <span class="competition-badge">USAMO</span>
                <span class="competition-badge">IMO</span>
                <span class="competition-badge">Putnam</span>
                <span class="competition-badge">USA TST</span>
                <span class="competition-badge">APMO</span>
                <span class="competition-badge">EGMO</span>
            </div>
            <p style="text-align: center; color: #999; margin-top: 10px;">Years 2022–2025</p>

            <h3 style="margin-top: 50px;">Expert Annotation Process</h3>
            <p>Creating reliable ground-truth labels required a rigorous two-stage annotation pipeline:</p>
            
            <p style="margin-top: 20px;"><strong>Stage 1: Marking Scheme Generation.</strong> For each problem, we use an LLM to automatically generate a problem-specific marking scheme that identifies key proof steps, assigns point values, and specifies deduction rules. Our expert annotators validated that approximately 85% of these generated schemes were of high quality and suitable for grading.</p>
            
            <p style="margin-top: 20px;"><strong>Stage 2: Expert Grading.</strong> Five experts with Putnam-level or national Math Olympiad experience grade each proof on a 0–7 scale. Crucially, experts treat the marking scheme as a detailed reference rather than a rigid checklist, allowing them to fairly credit valid alternative solution approaches. To ensure consistency, 40% of solutions were assigned to multiple annotators, achieving over 90% agreement.</p>

            <p style="margin-top: 25px;"><strong>Why 0–7?</strong> This scale aligns with the grading standards of premier mathematics competitions (our problem sources), enabling nuanced assessment of partial credit and proof quality—far beyond binary "correct/incorrect" judgments. Our results demonstrate that this fine-grained scoring is essential for effective downstream applications.</p>

            <p style="text-align: center; margin-top: 45px;">
                <a href="https://huggingface.co/datasets/wenjiema02/ProofBench" class="btn" style="font-size: 1.05em; padding: 14px 32px;">
                    Download ProofBench on HuggingFace
                </a>
            </p>

            <p style="margin-top: 30px; padding: 20px; background: #f9f9f9; border-radius: 4px; font-style: italic; color: #666; text-align: center;">
                → For complete dataset statistics, model performance breakdowns by competition, and details on the annotation pipeline, see <strong>Section 2</strong> of the paper.
            </p>
        </div>
    </section>

    <section id="contributions">
        <div class="container">
            <h2>Summary</h2>
            
            <p style="max-width: 900px; margin: 0 auto 40px; text-align: center; font-size: 1.1em;">
                This work addresses a critical bottleneck in mathematical proof generation by developing the first reliable, fine-grained evaluation methodology. Our contributions span dataset creation, systematic analysis, and practical validation.
            </p>

            <ul class="contributions">
                <li>
                    <strong>ProofBench Dataset:</strong> We introduce the first expert-annotated benchmark for fine-grained proof evaluation, containing 145 problems from six major mathematics competitions (2022–2025) and 435 solutions from state-of-the-art models (o3, Gemini-2.5-Pro, DeepSeek-R1). Each solution is graded on a 0–7 scale by experts with olympiad-level experience, following problem-specific marking schemes.
                </li>
                <li>
                    <strong>Systematic Design Space Exploration:</strong> We conduct the first comprehensive, training-free study of evaluator design factors. By systematically varying backbone models, context configurations, instruction styles, and workflows, we identify which factors matter most and establish actionable design principles for building reliable evaluators.
                </li>
                <li>
                    <strong>ProofGrader Evaluator:</strong> Combining insights from our study, we present ProofGrader—an evaluator that achieves MAE of 0.926 against expert scores. It combines a strong reasoning backbone (o3), rich context (reference solutions + marking schemes), flexible instructions, and simple ensembling for robustness.
                </li>
                <li>
                    <strong>Practical Validation:</strong> We demonstrate ProofGrader's utility beyond offline metrics through best-of-<em>n</em> selection experiments. At <em>n</em>=16, it closes 78% of the gap between a naive binary evaluator and human oracle performance, highlighting its potential as a reward signal for training proof generators.
                </li>
            </ul>

            <p style="margin-top: 40px; padding: 25px; background: #f9f9f9; border-radius: 4px; text-align: center;">
                <strong style="color: #333; font-size: 1.1em;">This website provides a high-level overview of our work.</strong><br>
                <span style="color: #666; font-size: 0.95em;">For complete experimental details, additional ablations, related work discussion, and future directions, please refer to the full paper.</span>
            </p>
        </div>
    </section>

    <footer>
        <div class="container">
            <p style="font-size: 1.1em; font-weight: 600; color: #444; margin-bottom: 15px;">Reliable Fine-Grained Evaluation of Natural Language Math Proofs</p>
            <p>UC Berkeley · Google DeepMind · Peking University · Allen Institute for AI</p>
            <p style="margin-top: 25px;">
                <a href="paper.pdf">Paper</a> · 
                <a href="https://huggingface.co/datasets/wenjiema02/ProofBench">Dataset</a> · 
                <a href="viewer.html">Viewer</a> · 
                <a href="mailto:windsey@berkeley.edu">Contact</a>
            </p>
        </div>
    </footer>
</body>
</html>
